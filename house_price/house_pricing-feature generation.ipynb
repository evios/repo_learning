{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup and Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_feather('tmp/house-after-cleaning')\n",
    "test_ID = pd.read_feather('tmp/house-testDF-idxs')\n",
    "\n",
    "#df_all = pd.read_feather('tmp/house-after-cleaning')\n",
    "#y_train_full= pd.read_feather('tmp/house-y-after-cleaning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Y separation, setting split values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dividing Target column (Y)\n",
    "y_train_full = df_raw.SalePrice.values\n",
    "df_all=df_raw.drop(['SalePrice'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 1460\n",
    "ntest = 1459\n",
    "y_train_full=y_train_full[:ntrain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Evaluation - benchmarking before Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m):\n",
    "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n",
    "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Making Training, Validation, Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def div_train_test(df):\n",
    "    \n",
    "    \"\"\"Dividing working DataFrame back to Train and Test\"\"\"\n",
    "    # split Validational/Test set from Training set after Categorical Value Engeneering\n",
    "    #def original_train_test(df_all):\n",
    "#    X_test=df.iloc[ntrain:] # Test set\n",
    "#    X_train_full=df.iloc[:ntrain] # Train set\n",
    "\n",
    "X_test=df_all.iloc[ntrain:] # Test set\n",
    "X_train_full=df_all.iloc[:ntrain] # Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all.shape, y_train_full.shape, X_test.shape, X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def quick_get_dumm(df):\n",
    "    X_train_full=df.iloc[:ntrain] # Train set\n",
    "#    X_train, X_valid, y_train, y_valid = train_test_split(pd.get_dummies(X_train_full), y_train_full)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(pd.get_dummies(X_train_full), y_train_full)\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_rf = RandomForestRegressor(n_estimators=160, min_samples_leaf=1, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_xgb = XGBRegressor(n_estimators=160, learning_rate=0.05)\n",
    "# using early_stop to find out where validation scores don't improve\n",
    "#m_xgb.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "%time m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({'feature': list(X_train.columns), 'importance':m_rf.feature_importances_}).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fi[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Ordinal values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding quality columns with dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Encode Quality columns with:\n",
    "Ex\tExcellent\n",
    "Gd\tGood\n",
    "TA\tAverage/Typical\n",
    "Fa\tFair\n",
    "Po\tPoor\n",
    "NA\tNo \"Garage/Basement/Fireplace/...\"\n",
    "\n",
    "To decode we use same Disctionary as used in other dataset columns:\n",
    "OverallCond: Rates the overall condition of the house\n",
    "       10\tVery Excellent\n",
    "       9\tExcellent\n",
    "       8\tVery Good\n",
    "       7\tGood\n",
    "       6\tAbove Average\t\n",
    "       5\tAverage\n",
    "       4\tBelow Average\t\n",
    "       3\tFair\n",
    "       2\tPoor\n",
    "       1\tVery Poor\n",
    "\"\"\"\n",
    "\n",
    "qual_cleanup = {\"Ex\": 9, \"Gd\": 7, \"TA\": 5, \"Fa\": 3,\"Po\": 2, \"None\": 0}\n",
    "\n",
    "for col in ('ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual','FireplaceQu','GarageQual',\n",
    "            'GarageCond','PoolQC'):\n",
    "    df_all[col].replace(qual_cleanup, inplace=True)\n",
    "    df_all[col]=df_all[col].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_all['BsmtCond'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['BsmtCond'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "BsmtFinType1: Rating of basement finished area\n",
    "\n",
    "       GLQ\tGood Living Quarters\n",
    "       ALQ\tAverage Living Quarters\n",
    "       BLQ\tBelow Average Living Quarters\t\n",
    "       Rec\tAverage Rec Room\n",
    "       LwQ\tLow Quality\n",
    "       Unf\tUnfinshed\n",
    "       NA\tNo Basement\n",
    "\"\"\"\n",
    "\n",
    "qual_cleanup = {\"GLQ\": 10, \"ALQ\": 8, \"BLQ\": 6, \"Rec\": 4, \"LwQ\": 3,\"Unf\": 2, \"None\": 0}\n",
    "\n",
    "for col in ('BsmtFinType1','BsmtFinType2'):\n",
    "    df_all[col].replace(qual_cleanup, inplace=True)\n",
    "    df_all[col]=df_all[col].astype(str)    \n",
    "\"\"\"\n",
    "BsmtExposure: Refers to walkout or garden level walls\n",
    "       Gd\tGood Exposure\n",
    "       Av\tAverage Exposure (split levels or foyers typically score average or above)\t\n",
    "       Mn\tMimimum Exposure\n",
    "       No\tNo Exposure\n",
    "       NA\tNo Basement\n",
    "\"\"\"\n",
    "qual_cleanup = {\"Gd\": 10, \"Av\": 7, \"Mn\": 4, \"No\": 2, \"None\": 0}\n",
    "\n",
    "df_all['BsmtExposure'].replace(qual_cleanup, inplace=True)\n",
    "df_all[col]=df_all[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_object_columns(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working on Functional (seems decrease score, not used now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_all['Functional'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Functional: Home functionality (Assume typical unless deductions are warranted)\n",
    "       Typ\tTypical Functionality\n",
    "       Min1\tMinor Deductions 1\n",
    "       Min2\tMinor Deductions 2\n",
    "       Mod\tModerate Deductions\n",
    "       Maj1\tMajor Deductions 1\n",
    "       Maj2\tMajor Deductions 2\n",
    "       Sev\tSeverely Damaged\n",
    "       Sal\tSalvage only\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "qual_cleanup = {\"Typ\": 10, \"Min1\": 9, \"Min2\": 8, \"Mod\": 6, \"Maj1\": 4,\"Maj2\": 3, \"Sev\": 1, \"Sal\": 0}\n",
    "\n",
    "df_all['Functional'].replace(qual_cleanup, inplace=True)\n",
    "df_all[col]=df_all[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Functional'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with GarageFinish (seems decrease score, not used now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_all['GarageFinish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_cleanup = {\"Fin\": 10, \"RFn\": 7, \"Unf\": 4, \"None\": 0}\n",
    "\n",
    "df_all['GarageFinish'].replace(qual_cleanup, inplace=True)\n",
    "df_all[col]=df_all[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_object_columns(df):\n",
    "    for col in df:\n",
    "        if is_string_dtype(df[col]):\n",
    "            print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_object_columns(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_object_columns(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transforming some numerical variables that are really categorical\n",
    "\n",
    "# MSSubClass=The building class\n",
    "\"\"\"\n",
    "MSSubClass: Identifies the type of dwelling involved in the sale.\t\n",
    "        20\t1-STORY 1946 & NEWER ALL STYLES\n",
    "        30\t1-STORY 1945 & OLDER\n",
    "        40\t1-STORY W/FINISHED ATTIC ALL AGES\n",
    "        45\t1-1/2 STORY - UNFINISHED ALL AGES\n",
    "        50\t1-1/2 STORY FINISHED ALL AGES\n",
    "        60\t2-STORY 1946 & NEWER\n",
    "        70\t2-STORY 1945 & OLDER\n",
    "        75\t2-1/2 STORY ALL AGES\n",
    "        80\tSPLIT OR MULTI-LEVEL\n",
    "        85\tSPLIT FOYER\n",
    "        90\tDUPLEX - ALL STYLES AND AGES\n",
    "       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n",
    "       150\t1-1/2 STORY PUD - ALL AGES\n",
    "       160\t2-STORY PUD - 1946 & NEWER\n",
    "       180\tPUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n",
    "       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES\n",
    "\"\"\"\n",
    "df_all['MSSubClass'] = df_all['MSSubClass'].astype(str)\n",
    "\n",
    "\n",
    "# Changing OverallCond into a categorical variable\n",
    "\"\"\"\n",
    "OverallCond: Rates the overall condition of the house\n",
    "       10\tVery Excellent\n",
    "       9\tExcellent\n",
    "       8\tVery Good\n",
    "       7\tGood\n",
    "       6\tAbove Average\t\n",
    "       5\tAverage\n",
    "       4\tBelow Average\t\n",
    "       3\tFair\n",
    "       2\tPoor\n",
    "       1\tVery Poor\n",
    "\"\"\"\n",
    "df_all['OverallCond'] = df_all['OverallCond'].astype(str)\n",
    "\n",
    "# Changing OverallQual into a categorical variable\n",
    "\"\"\"\n",
    "OverallQual: Rates the overall material and finish of the house\n",
    "       10\tVery Excellent\n",
    "       9\tExcellent\n",
    "       8\tVery Good\n",
    "       7\tGood\n",
    "       6\tAbove Average\n",
    "       5\tAverage\n",
    "       4\tBelow Average\n",
    "       3\tFair\n",
    "       2\tPoor\n",
    "       1\tVery Poor\n",
    "\"\"\"\n",
    "\n",
    "# Year and month sold are transformed into categorical features.\n",
    "df_all['YrSold'] = df_all['YrSold'].astype(str)\n",
    "df_all['MoSold'] = df_all['MoSold'].astype(str)\n",
    "\n",
    "df_all['YearBuilt']=df_all['YearBuilt'].astype(str)\n",
    "df_all['YearRemodAdd']=df_all['YearRemodAdd'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert object columns to categorical\n",
    "def conv_obj_to_categories(df):\n",
    "    \"\"\"\n",
    "    Convert Object columns to Categorical\n",
    "    \"\"\"\n",
    "    for col in df:\n",
    "        if is_string_dtype(df[col]):\n",
    "            df[col]=df[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#conv_obj_to_categories(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_categorical_columns(df):\n",
    "    \"\"\"\n",
    "    Print only categorical columns Number, Name and Codes of unique values in corresponding column \n",
    "    \"\"\"\n",
    "    for col in df:\n",
    "        if is_categorical_dtype(df[col]):\n",
    "            print(sum(np.unique(df[col].cat.categories,return_counts=True)[1]), col ,df[col].cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_categorical_columns(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def unique_categories(df,n=float(\"inf\")):\n",
    "    \"\"\"\n",
    "    Print only categorical columns Names and Number of unique values in corresponding column \n",
    "    df - DataFrame\n",
    "    n - show only columns with less then N unique values, \n",
    "        as default - not show column if more than 10000 unique value - not pseudo categorical\n",
    "    \"\"\"\n",
    "    for col in df:\n",
    "        if is_categorical_dtype(df[col]):\n",
    "            if sum(np.unique(df[col].cat.categories,return_counts=True)[1])<n:\n",
    "                print(col, sum(np.unique(df[col].cat.categories,return_counts=True)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_categories(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to int for work in feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in ('ExterCond','ExterQual','KitchenQual'):\n",
    "#    df_all[col]=df_all[col].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check numeric columns (if they are actually Categorical, like Year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting - heavily convert NUMERICAL to CATEGORICAL"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df_allcats=df_all.copy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Experimenting with Numerical Categories\n",
    "def conv_num_cat (df):\n",
    "    for col in df:\n",
    "        if is_numeric_dtype(df[col]): \n",
    "            df[col]=df[col].astype('category')\n",
    "        else:\n",
    "            df.drop(columns=col, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "conv_num_cat(df_allcats)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "unique_categories(df_allcats,20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "conv_to_cat_shortlist=['MSSubClass', 'MoSold','YrSold']#'OverallCond', 'OverallQual']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#for cat in conv_to_cat_longlist:\n",
    "#    df_all[cat]=df_all[cat].astype('category')\n",
    "\n",
    "for cat in conv_to_cat_shortlist:\n",
    "    df_all[cat]=df_all[cat].astype('category')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#conv_to_cat_longlist=['BedroomAbvGr', 'BsmtFullBath','BsmtHalfBath', 'Fireplaces', 'FullBath',\\\n",
    "#             'GarageCars','HalfBath','KitchenAbvGr','MSSubClass','MoSold','OverallCond',\\\n",
    "#             'OverallQual','PoolArea','TotRmsAbvGrd','YrSold']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all['TotalSF'] = df_all['TotalBsmtSF'] + df_all['1stFlrSF'] + df_all['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Age_Build']=df_all['YrSold'].astype(int)-df_all['YearBuilt'].astype(int)\n",
    "df_all['Age_Remod']=df_all['YrSold'].astype(int)-df_all['YearRemodAdd'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Sizes_Total']=df_all['GrLivArea']+df_all['GarageCars']+df_all['GarageArea']+df_all['TotalBsmtSF']+df_all['1stFlrSF']+df_all['2ndFlrSF']+df_all['OpenPorchSF']+df_all['MasVnrArea']\n",
    "df_all['Quantity_Total']=df_all['Fireplaces']+df_all['FullBath']+df_all['KitchenAbvGr']+df_all['TotRmsAbvGrd']+df_all['BedroomAbvGr']+df_all['BsmtFullBath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "#m_xgb.fit(X_train, y_train)\n",
    "#print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({'feature': list(X_train.columns), 'importance':m_rf.feature_importances_}).sort_values('importance',ascending=False)\n",
    "fi[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Garage_Age_Build']=df_all['YrSold'].astype(int)-df_all['GarageYrBlt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "#m_xgb.fit(X_train, y_train)\n",
    "#print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all['Quality_Aggregated']=df_all['ExterQual'].astype(int)+df_all['ExterCond'].astype(int)+df_all['BsmtQual'].astype(int)+df_all['BsmtCond'].astype(int)+df_all['KitchenQual'].astype(int)+df_all['OverallQual'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "#m_xgb.fit(X_train, y_train)\n",
    "#print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue Feature generation here\n",
    "#df_all['Basement']=df_all['TotalBsmtSF']+df_all['BsmtFinSF1']+df_all['BsmtFinSF2']-df_all['BsmtUnfSF'])*\n",
    "#(df_all['BsmtQual']+df_all['BsmtCond']+df_all['BsmtFinType1']+df_all['BsmtExposure']+df_all['BsmtFinType2'])*\n",
    "#df_all['BsmtFullBath']*0.5*df_all['BsmtHalfBath']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Garage=\n",
    "#House="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Crisis Data 2008-2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({'feature': list(X_train.columns), 'importance':m_rf.feature_importances_}).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fi[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_features_to_drop(X_train, X_valid, y_train, y_valid):\n",
    "    \"\"\" Using RandomForest identifies important feature \n",
    "    and one by one drop least important features from DataFrame to improve model score\n",
    "    input - X_train, X_valid, y_train, y_valid, same as used in training and evaluation model using train/valid split\n",
    "    \"\"\"\n",
    "    m_feature_to_drop = RandomForestRegressor(n_estimators=160, min_samples_leaf=1, max_features=0.5, n_jobs=-1, oob_score=False)\n",
    "    # to try - not use actual feature importance each iteration, but use only first one\n",
    "    #        m_feature_to_drop.fit(X_train, y_train)\n",
    "    #        fi = pd.DataFrame({'feature': list(X_train.columns), 'importance':m_feature_to_drop.feature_importances_}).sort_values('importance',ascending=False)\n",
    "    \n",
    "    # Number of features in DataFrame\n",
    "    num_of_features=X_train.shape[1]\n",
    "    \n",
    "    list_of_original_columns=X_train.columns\n",
    "    \n",
    "    best_grade=1\n",
    "    list_of_feature_to_drop=pd.DataFrame()\n",
    "    #grades={}\n",
    "    \n",
    "    for iteration in range(0, num_of_features):\n",
    "            \n",
    "        # Iteratively fit model with features without 1 least important (dropped in previos iteration)\n",
    "        m_feature_to_drop.fit(X_train, y_train)\n",
    "        # Evaluating performance withot this feature\n",
    "        grade=math.sqrt(mean_squared_error(y_valid, m_feature_to_drop.predict(X_valid)))\n",
    "\n",
    "        # Updating based on new model list of feature importance\n",
    "        fi = pd.DataFrame({'feature': list(X_train.columns), 'importance':m_feature_to_drop.feature_importances_}).sort_values('importance',ascending=False)\n",
    "\n",
    "        # Finding best score\n",
    "        if grade<best_grade:\n",
    "            best_grade=grade\n",
    "            best_num_of_features=(num_of_features-iteration)\n",
    "            list_of_feature_to_drop=list_of_original_columns.difference(fi.feature)\n",
    "\n",
    "        # Dropping last 1 (least important feature)\n",
    "        X_train=X_train.drop(columns=fi.feature[-1:])\n",
    "        X_valid=X_valid.drop(columns=fi.feature[-1:])\n",
    "\n",
    "        print ((num_of_features-iteration),grade, fi.feature[-1:])\n",
    "        #grades.update({(num_of_features-iteration):grade})\n",
    "    print(best_grade,best_num_of_features) \n",
    "    return list_of_feature_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#features_to_drop=find_features_to_drop(X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_to_drop\n",
    "#fi.feature==fi.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=list(grades.keys())\n",
    "y=list(grades.values())\n",
    "\n",
    "ax = plt.axes()\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ax = plt.axes()\n",
    "plt.xlim(150,300)\n",
    "plt.ylim(0.133,0.1350)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_all=df_all.drop(columns=features_to_drop)\n",
    "#df_all=df_all.drop(columns=fi.feature[150:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Dividing working DataFrame back to Train and Test\"\"\"\n",
    "# split Validational/Test set from Training set after Categorical Value Engeneering\n",
    "X_test=df_all.iloc[ntrain:] # Test set\n",
    "X_train_full=df_all.iloc[:ntrain] # Train set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Self made and experiment Evaluation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation of simple Random Forest\n",
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "%time m.fit(X_train, y_train)\n",
    "#print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "math.sqrt(mean_squared_error(y_valid, m.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if you need to evaluate LOG Root mean squared error but wouldn't like to convert y to log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "math.sqrt(mean_squared_log_error(np.expm1(y_valid), np.expm1(m.predict(X_valid))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> To delete outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"#check the numbers of samples and features\n",
    "print(\"The train data size before dropping Id feature is : {} \".format(df_train.shape))\n",
    "print(\"The test data size before dropping Id feature is : {} \".format(df_test.shape))\n",
    "\n",
    "#Save the 'Id' column\n",
    "train_ID = df_train['Id']\n",
    "test_ID = df_test['Id']\n",
    "\n",
    "#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "df_train.drop(\"Id\", axis = 1, inplace = True)\n",
    "df_test.drop(\"Id\", axis = 1, inplace = True)\n",
    "\n",
    "#check again the data size after dropping the 'Id' variable\n",
    "print(\"\\nThe train data size after dropping Id feature is : {} \".format(df_train.shape)) \n",
    "print(\"The test data size after dropping Id feature is : {} \".format(df_test.shape))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(df_train.corr())\n",
    "#plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(20, 20))\n",
    "\n",
    "sns.heatmap(df_train.corr(), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n",
    "sns.heatmap(df_test.corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n",
    "\n",
    "for i in range(2):    \n",
    "    axs[i].tick_params(axis='x', labelsize=14)\n",
    "    axs[i].tick_params(axis='y', labelsize=14)\n",
    "    \n",
    "axs[0].set_title('Training Set Correlations', size=15)\n",
    "axs[1].set_title('Test Set Correlations', size=15)\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df_corr=df_train.corr().sort_values(kind=\"quicksort\", ascending=False, by='SalePrice').abs()\n",
    "df_corr.drop(axis=1, columns=df_corr.columns.drop('SalePrice'), inplace=True)\n",
    "df_corr\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scewed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"numeric_feats = df_all.dtypes[df_all.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = df_all[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    #all_data[feat] += 1\n",
    "    df_all[feat] = boxcox1p(df_all[feat], lam)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Normalization, the Sigmoid, Log, Cube Root and the Hyperbolic Tangent. \n",
    "#It all depends on what one is trying to accomplish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import LabelEncoder\n",
    "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold')\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for c in cols:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(df_all[c].values)) \n",
    "    df_all[c] = lbl.transform(list(df_all[c].values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold')\n",
    "# process columns, apply BinaryEncoder to categorical features\n",
    "#for c in cols:\n",
    "#    bnr = ce.binary.BinaryEncoder() \n",
    "#    bnr.fit(list(df_all[c].values)) \n",
    "#    df_all[c] = bnr.transform(list(df_all[c].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all=pd.get_dummies(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Dividing working DataFrame back to Train and Test\"\"\"\n",
    "# split Validational/Test set from Training set after Categorical Value Engeneering\n",
    "#def original_train_test(df_all):\n",
    "X_test=df_all.iloc[ntrain:] # Test set\n",
    "X_train_full=df_all.iloc[:ntrain] # Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(pd.get_dummies(X_train_full), y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_rf = RandomForestRegressor(n_estimators=160, min_samples_leaf=1, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "# using early_stop to find out where validation scores don't improve\n",
    "m_xgb.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "%time m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT (Gradient Boosting Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_gbdt=GradientBoostingRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "%time m_gbdt.fit(X_train, y_train)\n",
    "print_score(m_gbdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing stacking from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.base import TransformerMixin,clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "n_folds=2\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train_full.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train_full.values, y_train_full, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, lasso),\n",
    "                                                 meta_model = lasso)\n",
    "\n",
    "score = rmsle_cv(stacked_averaged_models)\n",
    "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stacked_averaged_models.fit(X_train_full.values, y_train_full)\n",
    "stacked_train_pred = stacked_averaged_models.predict(X_train_full.values)\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(X_test.values))\n",
    "print(rmsle(y_train_full, stacked_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_xgb.fit(X_train_full, y_train_full)\n",
    "xgb_train_pred = m_xgb.predict(X_train_full)\n",
    "xgb_pred = np.expm1(m_xgb.predict(X_test))\n",
    "print(rmsle(y_train_full, xgb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_rf.fit(X_train_full, y_train_full)\n",
    "rf_train_pred = m_rf.predict(X_train_full)\n",
    "rf_pred = np.expm1(m_rf.predict(X_test.values))\n",
    "print(rmsle(y_train_full, rf_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''RMSE on the entire Train data when averaging'''\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "print(rmsle(y_train_full,stacked_train_pred*0.7 +\n",
    "               xgb_train_pred*0.15+rf_train_pred*0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = stacked_pred*0.7 +xgb_pred*0.15+rf_pred*0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_final_xgb = XGBRegressor(n_estimators=2000, learning_rate=0.05)\n",
    "m_final_xgb.fit(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.expm1(m_final_xgb.predict(X_test)); y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_ID.T.squeeze()\n",
    "sub['SalePrice'] = y_pred\n",
    "sub.to_csv('submittions/submission_29Aug19.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
