{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup and Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH = \"../input/house-prices-advanced-regression-techniques/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(f'{PATH}train.csv')#, index_col='Id')\n",
    "df_test=pd.read_csv(f'{PATH}test.csv')#, index_col='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y (target value) to Log, as stated at Kaggle Evaluation page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for the purpose of evaluation of current competition we transform target value\n",
    "df_train.SalePrice = np.log1p(df_train.SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of Training Examples = {}'.format(df_train.shape[0]))\n",
    "print('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\n",
    "print('Training X Shape = {}'.format(df_train.shape))\n",
    "print('Training y Shape = {}\\n'.format(df_train['SalePrice'].shape[0]))\n",
    "print('Test X Shape = {}'.format(df_test.shape))\n",
    "print('Test y Shape = {}\\n'.format(df_test.shape[0]))\n",
    "#print(df_train.columns)\n",
    "#print(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(df_train.info())\n",
    "#df_train.sample(3)\n",
    "#print(df_test.info())\n",
    "#df_test.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = df_train['GrLivArea'], y = df_train['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting outliers\n",
    "df_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)\n",
    "\n",
    "#Check the graphic again\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df_train['GrLivArea'], df_train['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame concatination and Y separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def concat_df(train_data, test_data):\n",
    "    # Returns a concatenated df of training and test set on axis 0\n",
    "    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n",
    "\n",
    "df_all = concat_df(df_train, df_test)\n",
    "\n",
    "df_train.name = 'Training Set'\n",
    "df_test.name = 'Test Set'\n",
    "df_all.name = 'All Set' \n",
    "\n",
    "dfs = [df_train, df_test]\n",
    "\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remember where to divide train and test\n",
    "ntrain = df_train.shape[0]\n",
    "ntest = df_test.shape[0]\n",
    "\n",
    "#Save the 'Id' column\n",
    "train_ID = df_train['Id']\n",
    "test_ID = df_test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dividing Target column (Y)\n",
    "y_train_full = df_train.SalePrice.values\n",
    "df_all.drop(['SalePrice'], axis=1, inplace=True)\n",
    "df_all.drop('Id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Create columns to mark originally missed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_missing (df):\n",
    "    for col in df.columns:\n",
    "        if df_all[col].isnull().sum()>0:\n",
    "            df_all[col+'_missed']=df_all[col].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_missing(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Replace Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_missing(df):\n",
    "    for col in df.columns:\n",
    "        print(col, df[col].isnull().sum())\n",
    "    print('\\n')\n",
    "    \n",
    "for df in dfs:\n",
    "    print(format(df.name))\n",
    "    display_missing(df)\n",
    "    \n",
    "    \n",
    "    \n",
    "#Check remaining missing values if any \n",
    "def display_only_missing(df):\n",
    "    all_data_na = (df.isnull().sum() / len(df)) * 100\n",
    "    all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "    missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "    print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_only_missing(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace non-missing but \"NA\", \"None\", etc values by Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace NA in Object columns, based on information from description"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Alley: Type of alley access to property\n",
    "       NA \tNo alley access\n",
    "MasVnrType: Masonry veneer type\n",
    "       None\tNone\n",
    "BsmtQual: Evaluates the height of the basement\n",
    "       NA\tNo Basement\n",
    "BsmtCond: Evaluates the general condition of the basement\n",
    "       NA\tNo Basement\n",
    "BsmtExposure: Refers to walkout or garden level walls\n",
    "       No\tNo Exposure\n",
    "       NA\tNo Basement\n",
    "BsmtFinType1: Rating of basement finished area\n",
    "       NA\tNo Basement\n",
    "BsmtFinType2: Rating of basement finished area (if multiple types)\n",
    "       NA\tNo Basement\n",
    "CentralAir: Central air conditioning\n",
    "       N\tNo\n",
    "FireplaceQu: Fireplace quality\n",
    "       NA\tNo Fireplace\n",
    "GarageType: Garage location\n",
    "       NA\tNo Garage\n",
    "GarageFinish: Interior finish of the garage\n",
    "       NA\tNo Garage\n",
    "GarageQual: Garage quality\n",
    "       NA\tNo Garage\n",
    "GarageCond: Garage condition\n",
    "       NA\tNo Garage\n",
    "PavedDrive: Paved driveway\n",
    "       N\tDirt/Gravel\n",
    "PoolQC: Pool quality\n",
    "       NA\tNo Pool\n",
    "Fence: Fence quality\n",
    "       NA\tNo Fence\n",
    "MiscFeature: Miscellaneous feature not covered in other categories\n",
    "       NA\tNone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA values (not missed) with None - based on data description -  - for non-Numerical (object) Columns\n",
    "for col in ('Alley','MasVnrType','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
    "            'BsmtFinType2','FireplaceQu','GarageType', 'GarageFinish', 'GarageQual', \n",
    "            'GarageCond','PoolQC','Fence','MiscFeature'):\n",
    "    df_all[col] = df_all[col].fillna('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace NA in Numerical columns, based on information from description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_only_missing(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fill NA numerical value with '0' - based on data description of correspondent Object columns - for Numerical Columns\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath','MasVnrArea'):\n",
    "    df_all[col] = df_all[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing real missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have REAL missing values, that we can't just replace based on description that if missed - use 'None' or 0. Hence, we will work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 1 - replacing by logic and deduction of human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace NA missing values by most often in column (only for columns with 2 and less NA values, where do not make sense to invest hugely into Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_only_missing(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill missing value in corresponding columns with most frequent value in column\n",
    "for col in ('Utilities','Functional','SaleType','KitchenQual','Exterior2nd','Exterior1st','Electrical'):\n",
    "    df_all[col].fillna(df_all[col].mode()[0], inplace=True)\n",
    "    \n",
    "# Functional : data description says NA means typical\n",
    "# BTW we just used df_all.Functional.mode() = use most frequent value (as 'Typ' is most frequent value)\n",
    "#df_all[\"Functional\"] = df_all[\"Functional\"].fillna(\"Typ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dealing with missing values left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_only_missing(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dealing with MSZoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.MSZoning.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MSZoning we have 4 missing values. \n",
    "We can replace them either by most common in column, or I have decided just with 'None' object values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\"\n",
    "MSZoning: Identifies the general zoning classification of the sale.\n",
    "       A\tAgriculture\n",
    "       C\tCommercial\n",
    "       FV\tFloating Village Residential\n",
    "       I\tIndustrial\n",
    "       RH\tResidential High Density\n",
    "       RL\tResidential Low Density\n",
    "       RP\tResidential Low Density Park \n",
    "       RM\tResidential Medium Density\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"MSZoning\"] = df_all[\"MSZoning\"].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_only_missing(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Iteration 2 - replacing by machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dealing with LotFrontage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['LotFrontage'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n",
    "df_all[\"LotFrontage\"] = df_all.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['LotFrontage'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_only_missing(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Seems no missed values\n",
    "Missing Values = DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Evaluation - benchmarking before Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Training, Validation, Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Dividing working DataFrame back to Train and Test\"\"\"\n",
    "# split Validational/Test set from Training set after Categorical Value Engeneering\n",
    "#X_test=df_all.iloc[ntrain:] # Test set\n",
    "X_train_full=df_all.iloc[:ntrain] # Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_all.shape, y_train_full.shape, X_test.shape, X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train, X_valid, y_train, y_valid = train_test_split(pd.get_dummies(X_train_full), y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting function (train/valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_get_dumm(df):\n",
    "    X_train_full=df.iloc[:ntrain] # Full Train set\n",
    "#    X_test=df_all.iloc[ntrain:] # Test set\n",
    "    \n",
    "    # Creating train and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(pd.get_dummies(X_train_full), y_train_full, random_state=42)\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = quick_get_dumm(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape, X_train_full.shape, y_train_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m,X_train=X_train, X_valid=X_valid, y_train=y_train, y_valid=y_valid):\n",
    "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n",
    "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_rf = RandomForestRegressor(n_estimators=160, min_samples_leaf=1, max_features=0.5, n_jobs=-1, oob_score=True, random_state=42)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_score(X,y):\n",
    "    lasso = ElasticNet(random_state=1)\n",
    "    param = {'l1_ratio' : [0],\n",
    "             'alpha' : [0.017]}\n",
    "    lasso = GridSearchCV(lasso, param, cv=5, scoring='neg_mean_squared_error')\n",
    "    lasso.fit(X,y)\n",
    "    print('Lasso:', np.sqrt(lasso.best_score_*-1))\n",
    "    return lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_xgb = XGBRegressor(n_estimators=160, learning_rate=0.05, random_state=42)\n",
    "# using early_stop to find out where validation scores don't improve\n",
    "#m_xgb.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "%time m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try ML techniques to predict all real missing values. We'll see how it will improve accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Once again dealing with missed LotFrontage feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We created function to return NA values of feature/column back in place, \n",
    "# based on _missed column, we created to state what values was missed in original dataset\n",
    "\n",
    "# returning original NA values back\n",
    "def return_original_na(df, feature):\n",
    "    df[feature].loc[df.index[df[feature+'_missed'] == True].tolist()]=np.nan\n",
    "    return df[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returning original NA values of MSZoning back in place\n",
    "df_all['LotFrontage']=return_original_na(df_all, 'LotFrontage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_only_missing(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filling_na_with_predictions(df, feature):\n",
    "    \"\"\"\n",
    "    df - DataFrame without target column y. Train+Test DataFrame (df_all)\n",
    "    feature - feature (column), containing real NA values we will fill\n",
    "\n",
    "    Assumption:\n",
    "    All other columns do not have NA values. In case of having we have to impute with some Statistical method (Median, etc)\n",
    "    We do not do it inside this function\n",
    "    \"\"\"\n",
    "\n",
    "    flag_object=0\n",
    "    \n",
    "    if df[feature].isnull().sum()>0:\n",
    "        ## Store Indexes of rows with NA values (we can just call \"_missed\" column with True values, to check those indexes as well)\n",
    "        ## Creating index based on NA values present in column\n",
    "        na_rows_idxs=df[df[feature].isnull()].index \n",
    "            ## Creating index based on NA values being present in original DF column\n",
    "            #na_rows_idxs=df.index[df[feature+'_missed'] == True].tolist()\n",
    "\n",
    "        ## For fitting and predictiong - convert DF to dummies DF, ready for ML\n",
    "        #df=pd.get_dummies(df)\n",
    "        ## If feature object we cant just dummy all, we shouldn't dummy feature column\n",
    "        df=pd.concat([ pd.Series(df[feature]), pd.get_dummies(df.drop([feature], axis=1)) ], axis=1)\n",
    "\n",
    "\n",
    "        ## Splitting DF to Feature_Train_X, Feature_Train_y, Feature_Predict_X:\n",
    "        ## Feature_Train_X = DF without NA values in \"feature_with_NA\"column\n",
    "        ## Feature_Train_y = target values that we have. All values in \"feature_with_NA\" except NA values\n",
    "        ## Feature_Predict_X = DF of correcponding to NA values in \"feature_with_NA\" without target vales (basically because they is equal to NA)\n",
    "        Feature_Train_X=df.drop(df[df[feature].isnull()].index).drop([feature], axis=1)\n",
    "        Feature_Train_y=df[feature].drop(df[df[feature].isnull()].index).values\n",
    "        Feature_Predict_X=df[df[feature].isnull()].drop([feature], axis=1)\n",
    "\n",
    "        ## If feature is NOT Numerical\n",
    "        ## Label encoding of y values in case it is not numerical\n",
    "        if is_string_dtype(df[feature]) or is_categorical_dtype(df[feature]):\n",
    "            flag_object=1\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le = LabelEncoder()\n",
    "            le.fit(Feature_Train_y)\n",
    "            Feature_Train_y=le.transform(Feature_Train_y)\n",
    "             \n",
    "        ## Making predictions, what might be in NA fields based on Train DF\n",
    "        #m_xgb = XGBRegressor(n_estimators=160, learning_rate=0.05)\n",
    "        #m_xgb.fit(Feature_Train_X, Feature_Train_y)\n",
    "        lasso = ElasticNet(random_state=1)\n",
    "        param = {'l1_ratio' : [0],\n",
    "             'alpha' : [0.017]}\n",
    "        lasso = GridSearchCV(lasso, param, cv=5, scoring='neg_mean_squared_error')\n",
    "        lasso.fit(Feature_Train_X,Feature_Train_y)\n",
    "    \n",
    "        ## Creating (Predicting) values to impute NA\n",
    "        #fillna_values=m_xgb.predict(Feature_Predict_X)\n",
    "        fillna_values=lasso.predict(Feature_Predict_X)\n",
    "\n",
    "        ## If feature is NOT Numerical\n",
    "        ## Return Encoded values back to Object/Category if feature NOT numerical\n",
    "        if flag_object==1:\n",
    "            fillna_values=le.inverse_transform(np.around(fillna_values).astype(int))\n",
    "        \n",
    "        ## Replacing NA values with predicted Series of values\n",
    "        df[feature]=df[feature].fillna(pd.Series(index=na_rows_idxs,data=fillna_values))\n",
    "\n",
    "        ## Returning feature column without NA values    \n",
    "        return df[feature]\n",
    "    else:\n",
    "        print ('There were no NA values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['LotFrontage']=filling_na_with_predictions(df_all, \"LotFrontage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df):\n",
    "    # Split dataset for train-validation\n",
    "    X_train, X_valid, y_train, y_valid = quick_get_dumm(df)\n",
    "    \n",
    "    #Lasso\n",
    "    lasso_score(X_train, y_train)\n",
    "\n",
    "    #XGBoost\n",
    "    m_xgb.fit(X_train, y_train)\n",
    "    print('XGBoost')\n",
    "    print_score(m_xgb,X_train, X_valid, y_train, y_valid)\n",
    "\n",
    "    # Random Forest\n",
    "    m_rf.fit(X_train, y_train)\n",
    "    print('Random Forest')\n",
    "    print_score(m_rf,X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! As we can see in all 3 models scores improved using ML algorithms to replace missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Once again dealing with missed MSZoning feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returning original NA values of MSZoning back in place\n",
    "df_all['MSZoning']=return_original_na(df_all, 'MSZoning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_only_missing(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all['MSZoning'].isnull()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['MSZoning']=filling_na_with_predictions(df_all, 'MSZoning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['MSZoning'].loc[df_all.index[df_all['MSZoning'+'_missed'] == True].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we had all 'RL' values for MSZoning column, but ML algorithms proposed to change it a little bit. Let's check score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Dealing with Missing values we replaced with most common - now replacing them with ML predictions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will deal with next features\n",
    "Utilities         0.068517\n",
    "Functional        0.068517\n",
    "SaleType          0.034258\n",
    "KitchenQual       0.034258\n",
    "Exterior2nd       0.034258\n",
    "Exterior1st       0.034258\n",
    "Electrical        0.034258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ('Utilities','Functional','SaleType','KitchenQual','Exterior2nd','Exterior1st','Electrical'):\n",
    "    print ('Filling with most common:\\n',df_all[col].loc[df_all.index[df_all[col+'_missed'] == True].tolist()])\n",
    "    df_all[col]=return_original_na(df_all, col)\n",
    "    df_all[col]=filling_na_with_predictions(df_all, col)\n",
    "    print ('Filling with predictions:\\n',df_all[col].loc[df_all.index[df_all[col+'_missed'] == True].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see - nothing in scores changed, so it was unnecessary step, possibly because these last features weren't important for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({'feature': list(X_train.columns), 'importance':m_rf.feature_importances_}).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated, unnessesary\n",
    "def select_encoding (df_all,encoding='onehot'):\n",
    "    if encoding=='label':\n",
    "        # Label Encoding\n",
    "        cols=[]\n",
    "        cols.extend(ordinal_features)\n",
    "        cols.extend(categorical_features)\n",
    "        cols.extend(df_all.select_dtypes(object).columns)\n",
    "        # process columns, apply LabelEncoder to categorical features\n",
    "        for c in cols:\n",
    "            if c in df_all.columns:\n",
    "                lbl = LabelEncoder() \n",
    "                lbl.fit(list(df_all[c].values)) \n",
    "                df_all[c] = lbl.transform(list(df_all[c].values))\n",
    "    if encoding=='binary':\n",
    "        # Binary Encoding\n",
    "        cols=[]\n",
    "        #cols.extend(ordinal_features)\n",
    "        cols.extend(categorical_features)\n",
    "        cols.extend(df_all.select_dtypes(object).columns)\n",
    "        # process columns, apply BinaryEncoder to categorical features\n",
    "        for c in cols:\n",
    "            if c in df_all.columns:\n",
    "                bnr = ce.binary.BinaryEncoder() \n",
    "                bnr.fit(list(df_all[c].values)) \n",
    "                df_all[c] = bnr.transform(list(df_all[c].values))\n",
    "    if encoding=='onehot':\n",
    "        df_all=pd.get_dummies(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_check_score(df):\n",
    "    X_train, X_valid, y_train, y_valid=quick_get_dumm(df)\n",
    "    # Lasso\n",
    "    print ('Lasso Score: ')\n",
    "    lasso_score(X_train, y_train)\n",
    "\n",
    "\"\"\"    # Random Forest\n",
    "    m_rf.fit(X_train, y_train)\n",
    "    print ('Random Forest Score: ')#; print_score(m_rf)\n",
    "    res = [rmse(m_rf.predict(X_train), y_train), rmse(m_rf.predict(X_valid), y_valid),\n",
    "                m_rf.score(X_train, y_train), m_rf.score(X_valid, y_valid)]\n",
    "    if hasattr(m_rf, 'oob_score_'): res.append(m_rf.oob_score_)\n",
    "    print(res)\n",
    "    \n",
    "    # XGBoost\n",
    "    m_xgb.fit(X_train, y_train)\n",
    "    print ('XGBoost Score: ')#; print_score(m_xgb)\n",
    "    res = [rmse(m_xgb.predict(X_train), y_train), rmse(m_xgb.predict(X_valid), y_valid),\n",
    "                m_xgb.score(X_train, y_train), m_xgb.score(X_valid, y_valid)]\n",
    "    #if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    \n",
    "    print(res)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_measure (df, feature):\n",
    "    enc=['ordinal','onehot','label','binary']#, 'BackwardDifferenceEncoder','HashingEncoder','HelmertEncoder','PolynomialEncoder']#'LeaveOneOutEncoder','TargetEncoder','WOEEncoder',\n",
    "    for encoding in enc:\n",
    "        if encoding=='ordinal':\n",
    "        # As Is encoding\n",
    "            df_ordinal=df.copy()\n",
    "            print (feature, 'Ordinal Encoding')\n",
    "            encoding_check_score(df_ordinal)\n",
    "        if encoding=='onehot':\n",
    "        # OneHot encoding\n",
    "            df_onehot=df.copy()\n",
    "            df_onehot[feature]=df_onehot[feature].astype(str)\n",
    "            df_onehot=pd.get_dummies(df_onehot)\n",
    "            print (feature, 'OneHot Encoding')\n",
    "            encoding_check_score(df_onehot)\n",
    "        if encoding=='label':\n",
    "        # Label Encoding\n",
    "            df_le=df.copy()\n",
    "            df_le[feature]=df_le[feature].astype(str)\n",
    "            lbl = LabelEncoder() \n",
    "            lbl.fit(list(df_le[feature].values)) \n",
    "            df_le[feature] = lbl.transform(list(df_le[feature].values))\n",
    "            print (feature, 'Label Encoding')\n",
    "            encoding_check_score(df_le)\n",
    "        if encoding=='binary':\n",
    "        # Binary Encoding\n",
    "            df_be=df.copy()\n",
    "            df_be[feature]=df_be[feature].astype(str)\n",
    "            bnr = ce.binary.BinaryEncoder() \n",
    "            bnr.fit(list(df_be[feature].values)) \n",
    "            df_be[feature] = bnr.transform(list(df_be[feature].values))\n",
    "            print (feature, 'Binary Encoding')\n",
    "            encoding_check_score(df_be)\n",
    "        if encoding=='LeaveOneOutEncoder':\n",
    "        # LeaveOneOutEncoder\n",
    "        #category_encoders.leave_one_out.LeaveOneOutEncoder\n",
    "            df_loo=df.copy()\n",
    "            df_loo[feature]=df_loo[feature].astype(str)\n",
    "            loo = ce.leave_one_out.LeaveOneOutEncoder() \n",
    "#            X_train_full=df_loo.iloc[:ntrain] # Train set\n",
    "            loo.fit(list(df_loo[feature].values),y_train_full) \n",
    "            df_loo[feature] = loo.transform(list(df_loo[feature].values))\n",
    "            print (feature, 'LeaveOneOutEncoder')\n",
    "            encoding_check_score(df_loo)\n",
    "        if encoding=='BackwardDifferenceEncoder':\n",
    "        # Backward Difference Coding\n",
    "        #category_encoders.backward_difference.BackwardDifferenceEncoder\n",
    "            df=df.copy()\n",
    "            df[feature]=df[feature].astype(str)\n",
    "            enc = ce.hashing.HashingEncoder() \n",
    "            enc.fit(list(df[feature].values)) \n",
    "            df[feature] = enc.transform(list(df[feature].values))\n",
    "            print (feature, encoding)\n",
    "            encoding_check_score(df)\n",
    "        if encoding=='HashingEncoder':\n",
    "        # Hashing\n",
    "        #category_encoders.hashing.HashingEncoder\n",
    "            df=df.copy()\n",
    "            df[feature]=df[feature].astype(str)\n",
    "            enc = ce.hashing.HashingEncoder() \n",
    "            enc.fit(list(df[feature].values)) \n",
    "            df[feature] = enc.transform(list(df[feature].values))\n",
    "            print (feature, encoding)\n",
    "            encoding_check_score(df)\n",
    "        if encoding=='HelmertEncoder':\n",
    "        # Helmert\n",
    "        #category_encoders.helmert.HelmertEncoder\n",
    "            df=df.copy()\n",
    "            df[feature]=df[feature].astype(str)\n",
    "            enc = ce.helmert.HelmertEncoder() \n",
    "            enc.fit(list(df[feature].values)) \n",
    "            df[feature] = enc.transform(list(df[feature].values))\n",
    "            print (feature, encoding)\n",
    "            encoding_check_score(df)        \n",
    "        if encoding=='PolynomialEncoder':\n",
    "        # Polinomial Coding\n",
    "        #category_encoders.polynomial.PolynomialEncoder\n",
    "            df=df.copy()\n",
    "            df[feature]=df[feature].astype(str)\n",
    "            enc = ce.polynomial.PolynomialEncoder() \n",
    "            enc.fit(list(df[feature].values)) \n",
    "            df[feature] = enc.transform(list(df[feature].values))\n",
    "            print (feature, encoding)\n",
    "            encoding_check_score(df)        \n",
    "        if encoding=='TargetEncoder':\n",
    "        # Target\n",
    "        #category_encoders.target_encoder.TargetEncoder\n",
    "            df=df.copy()\n",
    "            df[feature]=df[feature].astype(str)\n",
    "            enc = ce.target_encoder.TargetEncoder() \n",
    "#            X_train_full=df.iloc[:ntrain] # Train set\n",
    "            enc.fit(list(df[feature].values),y_train_full) \n",
    "            df[feature] = enc.transform(list(df[feature].values))\n",
    "            print (feature, encoding)\n",
    "            encoding_check_score(df)        \n",
    "        if encoding=='WOEEncoder':\n",
    "        #Weight of Evidence\n",
    "        #category_encoders.woe.WOEEncoder\n",
    "            df=df.copy()\n",
    "            df[feature]=df[feature].astype(str)\n",
    "            enc = ce.woe.WOEEncoder() \n",
    "            X_train_full=df.iloc[:ntrain] # Train set\n",
    "            enc.fit(list(X_train_full[feature].values),y_train_full) \n",
    "            df[feature] = enc.transform(list(df[feature].values))\n",
    "            print (feature, encoding, df.feature)\n",
    "#            encoding_check_score(df)\n",
    "        \n",
    "#        print ('\\n\\n')\n",
    "        #return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Ordinal values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding quality columns with dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Encode Quality columns with:\n",
    "Ex\tExcellent\n",
    "Gd\tGood\n",
    "TA\tAverage/Typical\n",
    "Fa\tFair\n",
    "Po\tPoor\n",
    "NA\tNo \"Garage/Basement/Fireplace/...\"\n",
    "\n",
    "To decode we use same Disctionary as used in other dataset columns:\n",
    "OverallCond: Rates the overall condition of the house\n",
    "       10\tVery Excellent\n",
    "       9\tExcellent\n",
    "       8\tVery Good\n",
    "       7\tGood\n",
    "       6\tAbove Average\t\n",
    "       5\tAverage\n",
    "       4\tBelow Average\t\n",
    "       3\tFair\n",
    "       2\tPoor\n",
    "       1\tVery Poor\n",
    "\"\"\"\n",
    "\n",
    "qual_cleanup = {\"Ex\": 9, \"Gd\": 7, \"TA\": 5, \"Fa\": 3,\"Po\": 2, \"None\": 0}\n",
    "\n",
    "# Checking/Evaluation effectiveness (error) of different encoding approaches (AsIs, OneHot, Label, Binary)\n",
    "for col in ('ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual',\n",
    "            'FireplaceQu','GarageQual','GarageCond','PoolQC'):\n",
    "    df_all_tmp=df_all.copy()\n",
    "    df_all_tmp[col].replace(qual_cleanup, inplace=True)\n",
    "    df_all_tmp[col]=df_all_tmp[col].astype(float)\n",
    "    encoding_measure (df_all_tmp, feature=col)\n",
    "    print ('---------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_features=[]\n",
    "onehot_features.append('KitchenQual')\n",
    "onehot_features.append('PoolQC')\n",
    "label_features=[]\n",
    "label_features.append('GarageQual')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.unique(df_all['BsmtCond'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all.ExterQual"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all['BsmtCond'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "BsmtFinType1: Rating of basement finished area\n",
    "BsmtFinType2: Rating of basement finished area (if multiple types)\n",
    "       GLQ\tGood Living Quarters\n",
    "       ALQ\tAverage Living Quarters\n",
    "       BLQ\tBelow Average Living Quarters\t\n",
    "       Rec\tAverage Rec Room\n",
    "       LwQ\tLow Quality\n",
    "       Unf\tUnfinshed\n",
    "       NA\tNo Basement\n",
    "\"\"\"\n",
    "\n",
    "qual_cleanup = {\"GLQ\": 10, \"ALQ\": 8, \"BLQ\": 6, \"Rec\": 4, \"LwQ\": 3,\"Unf\": 2, \"None\": 0}\n",
    "\n",
    "# Checking/Evaluation effectiveness (error) of different encoding approaches (AsIs, OneHot, Label, Binary)\n",
    "for col in ('BsmtFinType1','BsmtFinType2'):\n",
    "    df_all_tmp=df_all.copy()\n",
    "    df_all_tmp[col].replace(qual_cleanup, inplace=True)\n",
    "    df_all_tmp[col]=df_all_tmp[col].astype(float)    \n",
    "    encoding_measure (df_all_tmp, feature=col)\n",
    "    print ('---------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_features.append('BsmtFinType2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BsmtExposure: Refers to walkout or garden level walls\n",
    "       Gd\tGood Exposure\n",
    "       Av\tAverage Exposure (split levels or foyers typically score average or above)\t\n",
    "       Mn\tMimimum Exposure\n",
    "       No\tNo Exposure\n",
    "       NA\tNo Basement\n",
    "\"\"\"\n",
    "qual_cleanup = {\"Gd\": 10, \"Av\": 7, \"Mn\": 4, \"No\": 2, \"None\": 0}\n",
    "\n",
    "# Checking/Evaluation effectiveness (error) of different encoding approaches (AsIs, OneHot, Label, Binary)\n",
    "df_all_tmp=df_all.copy()\n",
    "df_all_tmp['BsmtExposure'].replace(qual_cleanup, inplace=True)\n",
    "df_all_tmp['BsmtExposure']=df_all_tmp['BsmtExposure'].astype(float)\n",
    "encoding_measure (df_all_tmp, feature='BsmtExposure')\n",
    "print ('---------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working on Functional (seems decrease score, not used now)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.unique(df_all['Functional'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Functional: Home functionality (Assume typical unless deductions are warranted)\n",
    "       Typ\tTypical Functionality\n",
    "       Min1\tMinor Deductions 1\n",
    "       Min2\tMinor Deductions 2\n",
    "       Mod\tModerate Deductions\n",
    "       Maj1\tMajor Deductions 1\n",
    "       Maj2\tMajor Deductions 2\n",
    "       Sev\tSeverely Damaged\n",
    "       Sal\tSalvage only\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "qual_cleanup = {\"Typ\": 10, \"Min1\": 9, \"Min2\": 8, \"Mod\": 6, \"Maj1\": 4,\"Maj2\": 3, \"Sev\": 1, \"Sal\": 0}\n",
    "\n",
    "# Checking/Evaluation effectiveness (error) of different encoding approaches (AsIs, OneHot, Label, Binary)\n",
    "df_all_tmp=df_all.copy()\n",
    "df_all_tmp['Functional'].replace(qual_cleanup, inplace=True)\n",
    "df_all_tmp['Functional']=df_all_tmp['Functional'].astype(float)\n",
    "encoding_measure (df_all_tmp, feature='Functional')\n",
    "print ('---------------------------')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all['Functional'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Working with Garage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.unique(df_all['GarageFinish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GarageFinish: Interior finish of the garage\n",
    "\n",
    "       Fin\tFinished\n",
    "       RFn\tRough Finished\t\n",
    "       Unf\tUnfinished\n",
    "       NA\tNo Garage\n",
    "\"\"\"\n",
    "\n",
    "qual_cleanup = {\"Fin\": 10, \"RFn\": 7, \"Unf\": 4, \"None\": 0}\n",
    "\n",
    "# Checking/Evaluation effectiveness (error) of different encoding approaches (AsIs, OneHot, Label, Binary)\n",
    "df_all_tmp=df_all.copy()\n",
    "df_all_tmp['GarageFinish'].replace(qual_cleanup, inplace=True)\n",
    "df_all_tmp['GarageFinish']=df_all_tmp['GarageFinish'].astype(float)\n",
    "encoding_measure (df_all_tmp, feature='GarageFinish')\n",
    "print ('---------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_features.append('GarageFinish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GarageType: Garage location\n",
    "\t\t\n",
    "       2Types\tMore than one type of garage\n",
    "       Attchd\tAttached to home\n",
    "       Basment\tBasement Garage\n",
    "       BuiltIn\tBuilt-In (Garage part of house - typically has room above garage)\n",
    "       CarPort\tCar Port\n",
    "       Detchd\tDetached from home\n",
    "       NA\tNo Garage\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "qual_cleanup = {\"2Types\": 10, \"Attchd\": 8, \"Basment\": 6, \"BuiltIn\": 4, \"CarPort\": 3,\"Detchd\": 2, \"None\": 0}\n",
    "\n",
    "# Checking/Evaluation effectiveness (error) of different encoding approaches (AsIs, OneHot, Label, Binary)\n",
    "df_all_tmp=df_all.copy()\n",
    "df_all_tmp['GarageType'].replace(qual_cleanup, inplace=True)\n",
    "df_all_tmp['GarageType']=df_all_tmp['GarageType'].astype(float)\n",
    "encoding_measure (df_all_tmp, feature='GarageType')\n",
    "print ('---------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_features.append('GarageType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dealing with BldgType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['BldgType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BldgType: Type of dwelling\n",
    "       1Fam\tSingle-family Detached\t\n",
    "       2FmCon\tTwo-family Conversion; originally built as one-family dwelling\n",
    "       Duplx\tDuplex\n",
    "       TwnhsE\tTownhouse End Unit\n",
    "       TwnhsI\tTownhouse Inside Unit\n",
    "\"\"\"\n",
    "\n",
    "qual_cleanup = {\"Twnhs\": 5, \"TwnhsE\": 4, \"Duplex\": 3, \"2fmCon\": 2, \"1Fam\": 1}\n",
    "\n",
    "# Checking/Evaluation effectiveness (error) of different encoding approaches (AsIs, OneHot, Label, Binary)\n",
    "df_all_tmp=df_all.copy()\n",
    "df_all_tmp['BldgType'].replace(qual_cleanup, inplace=True)\n",
    "df_all_tmp['BldgType']=df_all_tmp['BldgType'].astype(float)\n",
    "encoding_measure (df_all_tmp, feature='BldgType')\n",
    "print ('---------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dealing with HouseStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['HouseStyle'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "HouseStyle: Style of dwelling\n",
    "       1Story\tOne story\n",
    "       1.5Fin\tOne and one-half story: 2nd level finished\n",
    "       1.5Unf\tOne and one-half story: 2nd level unfinished\n",
    "       2Story\tTwo story\n",
    "       2.5Fin\tTwo and one-half story: 2nd level finished\n",
    "       2.5Unf\tTwo and one-half story: 2nd level unfinished\n",
    "       SFoyer\tSplit Foyer\n",
    "       SLvl\tSplit Level\n",
    "\"\"\"\n",
    "qual_cleanup = {\"SFoyer\":8,\"SLvl\":7,\"2.5Fin\":6,\"2.5Unf\": 5, \"2Story\": 4, \"1.5Fin\": 3, \"1.5Unf\": 2, \"1Story\": 1}\n",
    "\n",
    "# Checking/Evaluation effectiveness (error) of different encoding approaches (AsIs, OneHot, Label, Binary)\n",
    "df_all_tmp=df_all.copy()\n",
    "df_all_tmp['HouseStyle'].replace(qual_cleanup, inplace=True)\n",
    "df_all_tmp['HouseStyle']=df_all_tmp['HouseStyle'].astype(float)\n",
    "encoding_measure (df_all_tmp, feature='HouseStyle')\n",
    "print ('---------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dealing with Electrical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Electrical'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Electrical: Electrical system\n",
    "       SBrkr\tStandard Circuit Breakers & Romex\n",
    "       FuseA\tFuse Box over 60 AMP and all Romex wiring (Average)\t\n",
    "       FuseF\t60 AMP Fuse Box and mostly Romex wiring (Fair)\n",
    "       FuseP\t60 AMP Fuse Box and mostly knob & tube wiring (poor)\n",
    "       Mix\tMixed\n",
    "\"\"\"\n",
    "qual_cleanup = {\"SBrkr\": 5, \"FuseA\": 4, \"FuseF\": 3, \"FuseP\": 2, \"Mix\": 1}\n",
    "\n",
    "# Checking/Evaluation effectiveness (error) of different encoding approaches (AsIs, OneHot, Label, Binary)\n",
    "df_all_tmp=df_all.copy()\n",
    "df_all_tmp['Electrical'].replace(qual_cleanup, inplace=True)\n",
    "df_all_tmp['Electrical']=df_all_tmp['Electrical'].astype(float)\n",
    "encoding_measure (df_all_tmp, feature='Electrical')\n",
    "print ('---------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chosing encoding for ordinal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features=[]\n",
    "categorical_features=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosing Ordinal Encoding for Ordinal Data as most effective\n",
    "qual_cleanup = {\"Ex\": 9, \"Gd\": 7, \"TA\": 5, \"Fa\": 3,\"Po\": 2, \"None\": 0}\n",
    "\n",
    "#Ordinal encoding\n",
    "for col in ('ExterQual','ExterCond','BsmtQual','BsmtCond','KitchenQual',\n",
    "            'HeatingQC','FireplaceQu','GarageQual','GarageCond','PoolQC'):\n",
    "    ordinal_features.append(col)\n",
    "    df_all[col].replace(qual_cleanup, inplace=True)\n",
    "    df_all[col]=df_all[col].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosing Ordinal Encoding for Ordinal Data as most effective\n",
    "qual_cleanup = {\"GLQ\": 10, \"ALQ\": 8, \"BLQ\": 6, \"Rec\": 4, \"LwQ\": 3,\"Unf\": 2, \"None\": 0}\n",
    "\n",
    "for col in ('BsmtFinType1','BsmtFinType2'):\n",
    "    ordinal_features.append(col)\n",
    "    df_all[col].replace(qual_cleanup, inplace=True)\n",
    "    df_all[col]=df_all[col].astype(float)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosing Ordinal Encoding for Ordinal Data as most effective\n",
    "qual_cleanup = {\"Gd\": 10, \"Av\": 7, \"Mn\": 4, \"No\": 2, \"None\": 0}\n",
    "\n",
    "ordinal_features.append('BsmtExposure')\n",
    "df_all['BsmtExposure'].replace(qual_cleanup, inplace=True)\n",
    "df_all['BsmtExposure']=df_all['BsmtExposure'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosing Ordinal Encoding for Ordinal Data as most effective\n",
    "qual_cleanup = {\"Typ\": 10, \"Min1\": 9, \"Min2\": 8, \"Mod\": 6, \"Maj1\": 4,\"Maj2\": 3, \"Sev\": 1, \"Sal\": 0}\n",
    "\n",
    "ordinal_features.append('Functional')\n",
    "df_all['Functional'].replace(qual_cleanup, inplace=True)\n",
    "df_all['Functional']=df_all['Functional'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosing Ordinal Encoding for Ordinal Data as most effective\n",
    "qual_cleanup = {\"Fin\": 10, \"RFn\": 7, \"Unf\": 4, \"None\": 0}\n",
    "\n",
    "ordinal_features.append('GarageFinish')\n",
    "df_all['GarageFinish'].replace(qual_cleanup, inplace=True)\n",
    "df_all['GarageFinish']=df_all['GarageFinish'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosing Ordinal Encoding for Ordinal Data as most effective\n",
    "qual_cleanup = {\"2Types\": 10, \"Attchd\": 8, \"Basment\": 6, \"BuiltIn\": 4, \"CarPort\": 3,\"Detchd\": 2, \"None\": 0}\n",
    "\n",
    "ordinal_features.append('GarageType')\n",
    "df_all['GarageType'].replace(qual_cleanup, inplace=True)\n",
    "df_all['GarageType']=df_all['GarageFinish'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosing Ordinal Encoding for Ordinal Data as most effective\n",
    "qual_cleanup = {\"SFoyer\":8,\"SLvl\":7,\"2.5Fin\":6,\"2.5Unf\": 5, \"2Story\": 4, \"1.5Fin\": 3, \"1.5Unf\": 2, \"1Story\": 1}\n",
    "\n",
    "ordinal_features.append('HouseStyle')\n",
    "df_all['HouseStyle'].replace(qual_cleanup, inplace=True)\n",
    "df_all['HouseStyle']=df_all['HouseStyle'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosing Ordinal Encoding for Ordinal Data as most effective\n",
    "qual_cleanup = {\"SBrkr\": 5, \"FuseA\": 4, \"FuseF\": 3, \"FuseP\": 2, \"Mix\": 1}\n",
    "\n",
    "ordinal_features.append('Electrical')\n",
    "df_all['Electrical'].replace(qual_cleanup, inplace=True)\n",
    "df_all['Electrical']=df_all['Electrical'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = LabelEncoder() \n",
    "for feature in label_features:\n",
    "    # Label Encoding\n",
    "#    df_all[feature]=df_all[feature].astype(str)  \n",
    "#    lbl.fit(list(df_all[feature].values)) \n",
    "#    df_all[feature] = lbl.fit_transform(list(df_all[feature].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.GarageQual"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_object_columns(df):\n",
    "    for col in df:\n",
    "        if is_string_dtype(df[col]):\n",
    "            print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_object_columns(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features=[]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#CentralAir\n",
    "CentralAir_cleanup = {\"Y\": 1, \"N\": 0}\n",
    "\n",
    "df_all_tmp=df_all.copy()\n",
    "df_all_tmp['CentralAir'].replace(CentralAir_cleanup, inplace=True)\n",
    "#categorical_features.append('CentralAir')\n",
    "#df_all['CentralAir']=df_all['CentralAir'].astype(str)\n",
    "encoding_measure (df_all_tmp, feature='CentralAir')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Encoding CentralAir\n",
    "CentralAir_cleanup = {\"Y\": 1, \"N\": 0}\n",
    "\n",
    "categorical_features.append('CentralAir')\n",
    "df_all['CentralAir'].replace(CentralAir_cleanup, inplace=True)\n",
    "df_all['CentralAir']=df_all['CentralAir'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.MSSubClass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming some numerical variables that are really categorical\n",
    "\n",
    "# MSSubClass=The building class\n",
    "\"\"\"\n",
    "MSSubClass: Identifies the type of dwelling involved in the sale.\t\n",
    "        20\t1-STORY 1946 & NEWER ALL STYLES\n",
    "        30\t1-STORY 1945 & OLDER\n",
    "        40\t1-STORY W/FINISHED ATTIC ALL AGES\n",
    "        45\t1-1/2 STORY - UNFINISHED ALL AGES\n",
    "        50\t1-1/2 STORY FINISHED ALL AGES\n",
    "        60\t2-STORY 1946 & NEWER\n",
    "        70\t2-STORY 1945 & OLDER\n",
    "        75\t2-1/2 STORY ALL AGES\n",
    "        80\tSPLIT OR MULTI-LEVEL\n",
    "        85\tSPLIT FOYER\n",
    "        90\tDUPLEX - ALL STYLES AND AGES\n",
    "       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n",
    "       150\t1-1/2 STORY PUD - ALL AGES\n",
    "       160\t2-STORY PUD - 1946 & NEWER\n",
    "       180\tPUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n",
    "       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES\n",
    "\"\"\"\n",
    "encoding_measure (df_all, feature='MSSubClass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['MSSubClass'] = df_all['MSSubClass'].astype(str)\n",
    "categorical_features.append('MSSubClass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.MSSubClass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing OverallCond into a categorical variable\n",
    "\"\"\"\n",
    "OverallCond: Rates the overall condition of the house\n",
    "       10\tVery Excellent\n",
    "       9\tExcellent\n",
    "       8\tVery Good\n",
    "       7\tGood\n",
    "       6\tAbove Average\t\n",
    "       5\tAverage\n",
    "       4\tBelow Average\t\n",
    "       3\tFair\n",
    "       2\tPoor\n",
    "       1\tVery Poor\n",
    "\"\"\"\n",
    "encoding_measure (df_all, feature='OverallCond')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lblb_enc_features=[]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#df_all['OverallCond'] = df_all['OverallCond'].astype(str)\n",
    "#categorical_features.append('OverallCond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing OverallQual into a categorical variable\n",
    "\"\"\"\n",
    "OverallQual: Rates the overall material and finish of the house\n",
    "       10\tVery Excellent\n",
    "       9\tExcellent\n",
    "       8\tVery Good\n",
    "       7\tGood\n",
    "       6\tAbove Average\n",
    "       5\tAverage\n",
    "       4\tBelow Average\n",
    "       3\tFair\n",
    "       2\tPoor\n",
    "       1\tVery Poor\n",
    "\"\"\"\n",
    "encoding_measure (df_all, feature='OverallQual')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#df_all['OverallQual'] = df_all['OverallQual'].astype(str)\n",
    "#categorical_features.append('OverallQual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## still under question how to encode MoSold\n",
    "# Year and month sold are transformed into categorical features.\n",
    "#df_all['YrSold'] = df_all['YrSold'].astype(str)\n",
    "#df_all['MoSold'] = df_all['MoSold'].astype(str)\n",
    "#categorical_features.append('YrSold')\n",
    "encoding_measure (df_all, feature='YrSold')\n",
    "#categorical_features.append('MoSold')\n",
    "encoding_measure (df_all, feature='MoSold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all['YearBuilt']=df_all['YearBuilt'].astype(str)\n",
    "#categorical_features.append('YearBuilt')\n",
    "encoding_measure (df_all, feature='YearBuilt')\n",
    "\n",
    "#df_all['YearRemodAdd']=df_all['YearRemodAdd'].astype(str)\n",
    "#categorical_features.append('YearRemodAdd')\n",
    "encoding_measure (df_all, feature='YearRemodAdd')\n",
    "\n",
    "#df_all['GarageYrBlt']=df_all['GarageYrBlt'].astype(str)\n",
    "#categorical_features.append('GarageYrBlt')\n",
    "encoding_measure (df_all, feature='GarageYrBlt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.info(all)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# convert object columns to categorical\n",
    "def conv_obj_to_categories(df):\n",
    "    \"\"\"\n",
    "    Convert Object columns to Categorical\n",
    "    \"\"\"\n",
    "    for col in df:\n",
    "        if is_string_dtype(df[col]):\n",
    "            df[col]=df[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#conv_obj_to_categories(df_all)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def show_categorical_columns(df):\n",
    "    \"\"\"\n",
    "    Print only categorical columns Number, Name and Codes of unique values in corresponding column \n",
    "    \"\"\"\n",
    "    for col in df:\n",
    "        if is_categorical_dtype(df[col]):\n",
    "            print(sum(np.unique(df[col].cat.categories,return_counts=True)[1]), col ,df[col].cat.categories)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "show_categorical_columns(df_all)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def unique_categories(df,n=float(\"inf\")):\n",
    "    \"\"\"\n",
    "    Print only categorical columns Names and Number of unique values in corresponding column \n",
    "    df - DataFrame\n",
    "    n - show only columns with less then N unique values, \n",
    "        as default - not show column if more than 10000 unique value - not pseudo categorical\n",
    "    \"\"\"\n",
    "    for col in df:\n",
    "        if is_categorical_dtype(df[col]):\n",
    "            if sum(np.unique(df[col].cat.categories,return_counts=True)[1])<n:\n",
    "                print(col, sum(np.unique(df[col].cat.categories,return_counts=True)[1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "unique_categories(df_all)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scewed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "skewness = df_all.select_dtypes(include=numerics).apply(lambda x: skew(x))\n",
    "skew_index = skewness[abs(skewness) >= 0.75].index\n",
    "skewness[skew_index].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''BoxCox Transform'''\n",
    "lam = 0.15\n",
    "for column in skew_index:\n",
    "    df_all[column] = boxcox1p(df_all[column], lam)\n",
    "    #continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check numeric columns (if they are actually Categorical, like Year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting - heavily convert NUMERICAL to CATEGORICAL"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df_allcats=df_all.copy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Experimenting with Numerical Categories\n",
    "def conv_num_cat (df):\n",
    "    for col in df:\n",
    "        if is_numeric_dtype(df[col]): \n",
    "            df[col]=df[col].astype('category')\n",
    "        else:\n",
    "            df.drop(columns=col, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "conv_num_cat(df_allcats)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "unique_categories(df_allcats,20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "conv_to_cat_shortlist=['MSSubClass', 'MoSold','YrSold']#'OverallCond', 'OverallQual']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#for cat in conv_to_cat_longlist:\n",
    "#    df_all[cat]=df_all[cat].astype('category')\n",
    "\n",
    "for cat in conv_to_cat_shortlist:\n",
    "    df_all[cat]=df_all[cat].astype('category')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#conv_to_cat_longlist=['BedroomAbvGr', 'BsmtFullBath','BsmtHalfBath', 'Fireplaces', 'FullBath',\\\n",
    "#             'GarageCars','HalfBath','KitchenAbvGr','MSSubClass','MoSold','OverallCond',\\\n",
    "#             'OverallQual','PoolArea','TotRmsAbvGrd','YrSold']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using list of quntative and qualitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.select_dtypes(object).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature Importance Dropping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "fi = pd.DataFrame({'feature': list(X_train.columns), 'importance':m_rf.feature_importances_}).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "fi[:20]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "fi.tail(20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "df_all = df_all.drop(['Utilities', 'Street', 'PoolQC',], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "df_all = df_all.drop(['Utilities_missed','TotalBsmtSF_missed','SaleType_missed','MSZoning_missed',\n",
    "                      'KitchenQual_missed','GarageCars_missed','GarageArea_missed','Exterior2nd_missed',\n",
    "                     'Exterior1st_missed','BsmtFinSF2_missed','BsmtFullBath_missed','BsmtUnfSF_missed',\n",
    "                     'BsmtHalfBath_missed','Functional_missed'],  axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "df_all = df_all.drop(['Condition2','Heating','Exterior1st','BsmtFinSF1_missed','Foundation',\n",
    "                      'RoofMatl','Exterior2nd','Electrical_missed'],  axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "fi = pd.DataFrame({'feature': list(X_train.columns), 'importance':m_rf.feature_importances_}).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "fi[:50]\n",
    "#fi.tail(50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#df_all = df_all.drop(['Electrical','Heating','Exterior1st','BsmtFinSF1_missed','Foundation','RoofMatl','Exterior2nd','Electrical_missed'],  axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Finding which Features to Drop by function and visualisation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "def find_features_to_drop(X_train, X_valid, y_train, y_valid):\n",
    "    \"\"\" Using RandomForest identifies important feature \n",
    "    and one by one drop least important features from DataFrame to improve model score\n",
    "    input - X_train, X_valid, y_train, y_valid, same as used in training and evaluation model using train/valid split\n",
    "    \"\"\"\n",
    "    m_feature_to_drop = RandomForestRegressor(n_estimators=160, min_samples_leaf=1, max_features=0.5, n_jobs=-1, oob_score=False)\n",
    "    # to try - not use actual feature importance each iteration, but use only first one\n",
    "    #        m_feature_to_drop.fit(X_train, y_train)\n",
    "    #        fi = pd.DataFrame({'feature': list(X_train.columns), 'importance':m_feature_to_drop.feature_importances_}).sort_values('importance',ascending=False)\n",
    "    \n",
    "    # Number of features in DataFrame\n",
    "    num_of_features=X_train.shape[1]\n",
    "    \n",
    "    list_of_original_columns=X_train.columns\n",
    "    \n",
    "    best_grade=1\n",
    "    list_of_feature_to_drop=pd.DataFrame()\n",
    "    grades={}\n",
    "    \n",
    "    for iteration in range(0, num_of_features):\n",
    "            \n",
    "        # Iteratively fit model with features without 1 least important (dropped in previos iteration)\n",
    "        m_feature_to_drop.fit(X_train, y_train)\n",
    "        # Evaluating performance withot this feature\n",
    "        grade=math.sqrt(mean_squared_error(y_valid, m_feature_to_drop.predict(X_valid)))\n",
    "\n",
    "        # Updating based on new model list of feature importance\n",
    "        fi = pd.DataFrame({'feature': list(X_train.columns), 'importance':m_feature_to_drop.feature_importances_}).sort_values('importance',ascending=False)\n",
    "\n",
    "        # Finding best score\n",
    "        if grade<best_grade:\n",
    "            best_grade=grade\n",
    "            best_num_of_features=(num_of_features-iteration)\n",
    "            list_of_feature_to_drop=list_of_original_columns.difference(fi.feature)\n",
    "\n",
    "        # Dropping last 1 (least important feature)\n",
    "        X_train=X_train.drop(columns=fi.feature[-1:])\n",
    "        X_valid=X_valid.drop(columns=fi.feature[-1:])\n",
    "\n",
    "        print ((num_of_features-iteration),grade, fi.feature[-1:])\n",
    "        grades.update({(num_of_features-iteration):grade})\n",
    "    print(best_grade,best_num_of_features) \n",
    "    #return list_of_feature_to_drop\n",
    "    return grades"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "#features_to_drop=find_features_to_drop(X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "grades=find_features_to_drop(X_train, X_valid, y_train, y_valid)\n",
    "#features_to_drop\n",
    "#fi.feature==fi.feature"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "x=list(grades.keys())\n",
    "y=list(grades.values())\n",
    "\n",
    "ax = plt.axes()\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "\n",
    "ax = plt.axes()\n",
    "plt.xlim(150,300)\n",
    "plt.ylim(0.10,0.145)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "#df_all=df_all.drop(columns=features_to_drop)\n",
    "#df_all=df_all.drop(columns=fi.feature[150:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features generation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Age_Build']=df_all['YrSold'].astype(int)-df_all['YearBuilt'].astype(int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.drop(['Age_Build'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Age_Remod']=df_all['YrSold'].astype(int)-df_all['YearRemodAdd'].astype(int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.drop(['Age_Remod'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Sizes_Total']=df_all['GrLivArea']+df_all['GarageCars']+df_all['GarageArea']+df_all['TotalBsmtSF']+df_all['1stFlrSF']+df_all['2ndFlrSF']+df_all['OpenPorchSF']+df_all['MasVnrArea']\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all['Quantity_Total']=df_all['Fireplaces']+df_all['FullBath']+df_all['KitchenAbvGr']+df_all['TotRmsAbvGrd']+df_all['BedroomAbvGr']+df_all['BsmtFullBath']\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.drop(['Quantity_Total'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Garage_Age_Build']=df_all['YrSold'].astype(float)-df_all['GarageYrBlt'].astype(float)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all.drop(['Garage_Age_Build'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['YrBltAndRemod']=df_all['YearBuilt']+df_all['YearRemodAdd']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.drop(['YrBltAndRemod'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['TotalSF']=df_all['TotalBsmtSF'] + df_all['1stFlrSF'] + df_all['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Total_sqr_footage'] = (df_all['BsmtFinSF1'] + df_all['BsmtFinSF2'] +\n",
    "                                 df_all['1stFlrSF'] + df_all['2ndFlrSF'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all.drop(['Total_sqr_footage'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Total_Bathrooms'] = (df_all['FullBath'] + (0.5 * df_all['HalfBath']) +\n",
    "                               df_all['BsmtFullBath'] + (0.5 * df_all['BsmtHalfBath']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all.drop(['Total_Bathrooms'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Total_porch_sf'] = (df_all['OpenPorchSF'] + df_all['3SsnPorch'] +\n",
    "                              df_all['EnclosedPorch'] + df_all['ScreenPorch'] +\n",
    "                              df_all['WoodDeckSF'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all['haspool'] = df_all['PoolArea'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.drop(['haspool'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['has2ndfloor'] = df_all['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all.drop(['has2ndfloor'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['hasgarage'] = df_all['GarageArea'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all.drop(['hasgarage'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['hasbsmt'] = df_all['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all.drop(['hasbsmt'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['hasfireplace'] = df_all['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.drop(['hasfireplace'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Basement']=(df_all['TotalBsmtSF']+df_all['BsmtFinSF1']+df_all['BsmtFinSF2']-df_all['BsmtUnfSF'])*(df_all['BsmtQual']+df_all['BsmtCond'].astype(int)+df_all['BsmtFinType1']+df_all['BsmtExposure'].astype(int)+df_all['BsmtFinType2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all.drop(['Basement'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all['Garage']=(df_all['GarageArea'])*(df_all['GarageQual']+df_all['GarageCond']+df_all['GarageType'])*df_all['GarageCars']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all.drop(['Garage'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#House="
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data examining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.info(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.select_dtypes(object).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Crisis Data 2008-2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shiller index Monthly"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Case-Shiller U.S. National Home Price Index (CSUSHPISA)\n",
    "csi=pd.read_csv(f'{PATH}CSUSHPISA.csv')#, index_col='Id')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "csi['DATE']=pd.to_datetime(csi['DATE'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "csi['YrSold']=csi.DATE.dt.year\n",
    "csi['MoSold']=csi.DATE.dt.month\n",
    "csi.drop(['DATE'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "csi.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all=df_all.merge(csi, how='left')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all[['YrSold','MoSold','CSUSHPISA']].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encoding_measure (df_all, feature='CSUSHPISA')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#df_all['CSUSHPISA']=df_all['CSUSHPISA'].astype(str)\n",
    "#categorical_features.append('CSUSHPISA')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#df_all.drop(['CSUSHPISA'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### HPI index Quarterly"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# All-Transactions House Price Index for the United States\n",
    "hpi=pd.read_csv(f'{PATH}USSTHPI.csv')#, index_col='Id')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hpi['DATE']=pd.to_datetime(hpi['DATE'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hpi['YrSold']=hpi.DATE.dt.year\n",
    "hpi['MoSold']=hpi.DATE.dt.month\n",
    "hpi.drop(['DATE'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hpi.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all=df_all.merge(hpi, how='left')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all[['YrSold','MoSold','USSTHPI']].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#df_all['USSTHPI']=df_all['USSTHPI'].astype(str)\n",
    "#categorical_features.append('USSTHPI')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all.drop(['USSTHPI'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Purchase Only House Price Index for the United States (Monthly)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Purchase Only House Price Index for the United States (Monthly)\n",
    "pi=pd.read_csv(f'{PATH}HPIPONM226S.csv')#, index_col='Id')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pi['DATE']=pd.to_datetime(pi['DATE'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pi['YrSold']=pi.DATE.dt.year\n",
    "pi['MoSold']=pi.DATE.dt.month\n",
    "pi.drop(['DATE'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pi.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all=df_all.merge(pi, how='left')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all[['YrSold','MoSold','HPIPONM226S']].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#df_all['HPIPONM226S']=df_all['HPIPONM226S'].astype(str)\n",
    "#categorical_features.append('HPIPONM226S')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all.drop(['HPIPONM226S'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Case-Shiller 20-City Home Price Sales Pair Counts (Monthly)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Case-Shiller 20-City Home Price Sales Pair Counts (Monthly)\n",
    "hsi=pd.read_csv(f'{PATH}SPCS20RPSNSA.csv')#, index_col='Id')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hsi['DATE']=pd.to_datetime(hsi['DATE'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hsi['YrSold']=hsi.DATE.dt.year\n",
    "hsi['MoSold']=hsi.DATE.dt.month\n",
    "hsi.drop(['DATE'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hsi.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all=df_all.merge(hsi, how='left')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all[['YrSold','MoSold','SPCS20RPSNSA']].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "m_rf.fit(X_train, y_train)\n",
    "print_score(m_rf)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#df_all.drop(['SPCS20RPSNSA'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#df_all['SPCS20RPSNSA']=df_all['SPCS20RPSNSA'].astype(str)\n",
    "#categorical_features.append('SPCS20RPSNSA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def select_encoding (df_all,encoding='onehot'):\n",
    "    if encoding=='label':\n",
    "        # Label Encoding\n",
    "        cols=[]\n",
    "        cols.extend(ordinal_features)\n",
    "        cols.extend(categorical_features)\n",
    "        cols.extend(df_all.select_dtypes(object).columns)\n",
    "        # process columns, apply LabelEncoder to categorical features\n",
    "        for c in cols:\n",
    "            if c in df_all.columns:\n",
    "                lbl = LabelEncoder() \n",
    "                lbl.fit(list(df_all[c].values)) \n",
    "                df_all[c] = lbl.transform(list(df_all[c].values))\n",
    "    if encoding=='binary':\n",
    "        # Binary Encoding\n",
    "        cols=[]\n",
    "        #cols.extend(ordinal_features)\n",
    "        cols.extend(categorical_features)\n",
    "        cols.extend(df_all.select_dtypes(object).columns)\n",
    "        # process columns, apply BinaryEncoder to categorical features\n",
    "        for c in cols:\n",
    "            if c in df_all.columns:\n",
    "                bnr = ce.binary.BinaryEncoder() \n",
    "                bnr.fit(list(df_all[c].values)) \n",
    "                df_all[c] = bnr.transform(list(df_all[c].values))\n",
    "    if encoding=='onehot':\n",
    "        df_all=pd.get_dummies(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df_all=select_encoding(df_all,'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all=pd.get_dummies(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Dividing working DataFrame back to Train and Test\"\"\"\n",
    "# split Validational/Test set from Training set after Categorical Value Engeneering\n",
    "#def original_train_test(df_all):\n",
    "X_test=df_all.iloc[ntrain:] # Test set\n",
    "X_train_full=df_all.iloc[:ntrain] # Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(pd.get_dummies(X_train_full), y_train_full)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping low variance features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\"\n",
    "Features that have too low variance can negatively impact the model, so we need to remove them by the number of repetitive equal values. In this case, we used a threshold of 99.2% (not 0 or 1 values). Therefore, if any feature has more than 99.2% reps of 1 or 0 it will be excluded. When doing this,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all features for future comparison.\n",
    "all_features = df_all.keys()\n",
    "# Removing features.\n",
    "df_all = df_all.drop(df_all.loc[:,(df_all==0).sum()>=(df_all.shape[0]*0.992)],axis=1)\n",
    "df_all = df_all.drop(df_all.loc[:,(df_all==1).sum()>=(df_all.shape[0]*0.992)],axis=1) \n",
    "# Getting and printing the remaining features.\n",
    "remain_features = df_all.keys()\n",
    "remov_features = [st for st in all_features if st not in remain_features]\n",
    "print(len(remov_features), 'features were removed:', remov_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Normalization, the Sigmoid, Log, Cube Root and the Hyperbolic Tangent. \n",
    "#It all depends on what one is trying to accomplish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.RobustScaler()\n",
    "df_all = pd.DataFrame(scaler.fit_transform(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid=quick_get_dumm(df_all)\n",
    "lasso_score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = df_all.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all ,[] symbols from dataframe columns and values\n",
    "#df_all.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in df_all.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dividing working DataFrame back to Train and Test\"\"\"\n",
    "# split Validational/Test set from Training set after Categorical Value Engeneering\n",
    "#def original_train_test(df_all):\n",
    "X_test=df_all.iloc[ntrain:] # Test set\n",
    "X_train_full=df_all.iloc[:ntrain] # Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(pd.get_dummies(X_train_full), y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_xgb.fit(X_train, y_train)\n",
    "print_score(m_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train():\n",
    "    lasso = ElasticNet(random_state=1)\n",
    "    param = {'l1_ratio' : [0],\n",
    "             'alpha' : [0.017]}\n",
    "    lasso = GridSearchCV(lasso, param, cv=5, scoring='neg_mean_squared_error')\n",
    "    lasso.fit(X_train_full, y_train_full)\n",
    "    print('Lasso:', np.sqrt(lasso.best_score_*-1))\n",
    "    return lasso\n",
    "lasso = cv_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.expm1(lasso.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_ID\n",
    "sub['SalePrice'] = y_pred\n",
    "sub.to_csv('submittions/submission_31Aug19.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "m_lasso_1 = ElasticNet(random_state=1, alpha=0.017)\n",
    "m_lasso_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "m_lasso_2 = ElasticNet(random_state=1, alpha=0.017)\n",
    "m_lasso_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preds_lasso_1=m_lasso_1.predict(X_valid)\n",
    "preds_lasso_2=m_lasso_2.predict(X_valid)\n",
    "\n",
    "test_preds_lasso_1=m_lasso_1.predict(X_test)\n",
    "test_preds_lasso_2=m_lasso_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stacked_predictions=np.column_stack((preds_lasso_1,preds_lasso_2))\n",
    "stacked_test_predictions=np.column_stack((test_preds_lasso_1,test_preds_lasso_2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "meta_model=ElasticNet(random_state=1, alpha=0.017)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "meta_model.fit(stacked_predictions,y_valid)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# testing of very simple stacking\n",
    "y_pred = np.expm1(meta_model.predict(stacked_test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
